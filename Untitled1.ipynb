{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Jmj1YIZmh9u",
        "outputId": "f18d09d6-f70d-4bd5-fce0-1563c91463d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural Language Processing (NLP) is a field of artificial intelligence that focuses on enabling computers to understand, interpret, and generate human language, both written and spoken. It combines computer science, linguistics, and artificial intelligence to bridge the gap between human communication and machine understanding. \n"
          ]
        }
      ],
      "source": [
        "with open('nlp.txt', 'r', encoding='utf-8') as file:\n",
        "  content = file.read()\n",
        "print(content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "reader = PdfReader(\"nlp.pdf\")\n",
        "for page_num, page in enumerate(reader.pages, start=1):\n",
        "    text = page.extract_text()\n",
        "    print(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTeuMvF1q1xC",
        "outputId": "d718b2da-7182-4530-83af-36d4743fdf4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural Language Processing (NLP) is  a field of artificial intelligence that focuses on enabling \n",
            "computers to understand, interpret, and generate human language, both written and \n",
            "spoken.  It combines computer science, linguistics, and artificial intelligenc e to bridge the gap \n",
            "between human communication and machine understanding.   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import re\n",
        "with zipfile.ZipFile('nlp.docx') as docx:\n",
        "  with docx.open(\"word/document.xml\") as xml_file:\n",
        "    xml_content = xml_file.read().decode(\"utf-8\")\n",
        "    text = re.sub(r\"<[^>]+>\", \"\", xml_content)\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dR4uUmzdq_tk",
        "outputId": "08bd7284-7abb-4712-e76e-737e228f6002"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\n",
            "Natural Language Processing (NLP) is a field of artificial intelligence that focuses on enabling computers to understand, interpret, and generate human language, both written and spoken. It combines computer science, linguistics, and artificial intelligence to bridge the gap between human communication and machine understanding. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "url = \"https://en.wikipedia.org/wiki/Python_(programming_language)\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "paragraphs = soup.select(\"div.mw-parser-output > p\")\n",
        "article_text = \"\\n\".join(p.get_text() for p in paragraphs)\n",
        "print(article_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfCFdG6pr867",
        "outputId": "f0e18965-3d21-478d-dabb-00f7afd729b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.[34]\n",
            "\n",
            "Python is dynamically type-checked and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming.\n",
            "\n",
            "Guido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language, and he first released it in 1991 as Python 0.9.0.[35] Python 2.0 was released in 2000. Python 3.0, released in 2008, was a major revision not completely backward-compatible with earlier versions. Python 2.7.18, released in 2020, was the last release of Python 2.[36]\n",
            "\n",
            "Python consistently ranks as one of the most popular programming languages, and it has gained widespread use in the machine learning community.[37][38][39][40] It is widely taught as an introductory programming language.[41]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    text = text.lower()\n",
        "    cleaned = \"\"\n",
        "    for ch in text:\n",
        "        if ch.isalpha() or ch.isspace():\n",
        "            cleaned += ch\n",
        "        else:\n",
        "            cleaned += \" \"\n",
        "    return [word for word in cleaned.split(\" \") if word]\n",
        "def stem(word):\n",
        "    suffixes = [\"ing\", \"ed\", \"ly\", \"es\", \"s\"]\n",
        "    for suf in suffixes:\n",
        "        if word.endswith(suf) and len(word) > len(suf) + 1:\n",
        "            return word[:-len(suf)]\n",
        "    return word\n",
        "def lemmatize(word):\n",
        "    lemmas  = {\n",
        "      \"we\": \"we\",\n",
        "      \"are\": \"be\",\n",
        "      \"studying\": \"study\",\n",
        "      \"nlp\": \"nlp\",\n",
        "      \"in\": \"in\",\n",
        "      \"bangalore\": \"bangalore\",\n",
        "    }\n",
        "    if word in lemmas:\n",
        "        return lemmas[word]\n",
        "    if word.endswith(\"ies\") and len(word) > 4:\n",
        "        return word[:-3] + \"y\"\n",
        "    elif word.endswith(\"s\") and len(word) > 3:\n",
        "        return word[:-1]\n",
        "\n",
        "    return word\n",
        "text = \"we are studying nlp in bangalore\"\n",
        "tokens = tokenize(text)\n",
        "stems = [stem(w) for w in tokens]\n",
        "lemmas = [lemmatize(w) for w in tokens]\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Stems:\", stems)\n",
        "print(\"Lemmas:\", lemmas)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URGzewsJvICx",
        "outputId": "8750a25e-1c96-432a-fef6-d5305aded68f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['we', 'are', 'studying', 'nlp', 'in', 'bangalore']\n",
            "Stems: ['we', 'are', 'study', 'nlp', 'in', 'bangalore']\n",
            "Lemmas: ['we', 'be', 'study', 'nlp', 'in', 'bangalore']\n"
          ]
        }
      ]
    }
  ]
}