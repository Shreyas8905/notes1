{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6ec86cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagger(corpus):\n",
    "    words = corpus.lower().split()\n",
    "    tagged = []\n",
    "    for i, word in enumerate(words):\n",
    "        tag = \"NN\" \n",
    "        if word.endswith(\"ing\"):\n",
    "            tag = \"VBG\" \n",
    "        elif word.endswith(\"ed\"):\n",
    "            tag = \"VBD\"  \n",
    "        elif word.endswith(\"ly\"):\n",
    "            tag = \"RB\"  \n",
    "        elif word in [\"a\", \"an\", \"the\"]:\n",
    "            tag = \"DT\" \n",
    "        elif word in [\"i\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\"]:\n",
    "            tag = \"PRP\" \n",
    "        elif word in [\"from\", \"in\", \"of\"]:\n",
    "            tag = \"PRE\" \n",
    "        elif word.replace('.', '', 1).isdigit():\n",
    "            tag = \"CD\" \n",
    "        elif i > 0 and tagged[i-1][1] == \"DT\" and not word.endswith(\"ing\"): \n",
    "            tag = \"NN\" \n",
    "        elif word in [\"is\", \"am\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\"]:\n",
    "            tag = \"VBZ\"\n",
    "        tagged.append((word, tag))\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f75ce97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 'DT'),\n",
       " ('chirping', 'VBG'),\n",
       " ('of', 'PRE'),\n",
       " ('birds', 'NN'),\n",
       " ('called', 'VBD'),\n",
       " ('forth', 'NN'),\n",
       " ('the', 'DT'),\n",
       " ('sun,', 'NN'),\n",
       " ('rising', 'VBG'),\n",
       " ('from', 'PRE'),\n",
       " ('the', 'DT'),\n",
       " ('horizon', 'NN'),\n",
       " ('in', 'PRE'),\n",
       " ('likeness', 'NN'),\n",
       " ('of', 'PRE'),\n",
       " ('a', 'DT'),\n",
       " ('phoenix.', 'NN')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = \"The chirping of birds called forth the sun, rising from the horizon in likeness of a phoenix.\"\n",
    "pos_tagger(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec6b156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_noun(word):\n",
    "    common_nouns = {'fox', 'dog'}\n",
    "    return word in common_nouns\n",
    "def is_verb(word):\n",
    "    common_verbs = {'jumped'}\n",
    "    return word in common_verbs\n",
    "def is_adjective(word):\n",
    "    common_adjectives = {'quick', 'brown', 'lazy'}\n",
    "    return word in common_adjectives\n",
    "def is_preposition(word):\n",
    "    common_prepositions = {'over'}\n",
    "    return word in common_prepositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5dde08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_noun_phrases(sentence):\n",
    "    words = sentence.lower().split()\n",
    "    noun_phrases = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        if words[i] in ['the', 'a', 'an']:\n",
    "            np_words = [words[i]]\n",
    "            i += 1\n",
    "            while i < len(words) and (is_adjective(words[i]) or is_noun(words[i])):\n",
    "                np_words.append(words[i])\n",
    "                i += 1\n",
    "            noun_phrases.append(' '.join(np_words))\n",
    "        else:\n",
    "            i += 1    \n",
    "    return noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9ca0cd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_verb_phrases(sentence):\n",
    "    words = sentence.lower().split()\n",
    "    verb_phrases = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        if is_verb(words[i]):\n",
    "            vp_words = [words[i]]\n",
    "            i += 1\n",
    "            while i < len(words) and (is_preposition(words[i]) or words[i] in ['over', 'to', 'for']):\n",
    "                vp_words.append(words[i])\n",
    "                i += 1\n",
    "            verb_phrases.append(' '.join(vp_words))\n",
    "        else:\n",
    "            i += 1\n",
    "    return verb_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1b24c322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun Phrases:\n",
      "1. the quick brown fox\n",
      "2. the lazy dog\n",
      "Verb Phrases:\n",
      "1. jumped over\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The quick brown fox jumped over the lazy dog\"\n",
    "print(\"Noun Phrases:\")\n",
    "noun_phrases = find_noun_phrases(sentence)\n",
    "for i, np in enumerate(noun_phrases, 1):\n",
    "    print(f\"{i}. {np}\")\n",
    "print(\"Verb Phrases:\")\n",
    "verb_phrases = find_verb_phrases(sentence)\n",
    "for i, vp in enumerate(verb_phrases, 1):\n",
    "    print(f\"{i}. {vp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "412f0a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['cat', 'is', 'sitting', 'the']\n",
      "Token 1 BoW Vector: [0, 0, 0, 1]\n",
      "Token 2 BoW Vector: [1, 0, 0, 0]\n",
      "Token 3 BoW Vector: [0, 1, 0, 0]\n",
      "Token 4 BoW Vector: [0, 0, 1, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "corpus = \"The cat is sitting\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = word_tokenize(corpus.lower())\n",
    "vocabulary = sorted(set(tokens))\n",
    "\n",
    "# Build BoW for each token\n",
    "bow_vectors = []\n",
    "for word in tokens:\n",
    "    vector = [0] * len(vocabulary)\n",
    "    vector[vocabulary.index(word)] = 1\n",
    "    bow_vectors.append(vector)\n",
    "\n",
    "# Print results\n",
    "print(\"Vocabulary:\", vocabulary)\n",
    "for i, vector in enumerate(bow_vectors, 1):\n",
    "    print(f\"Token {i} BoW Vector: {vector}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
